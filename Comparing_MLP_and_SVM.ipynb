{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#BUILDING MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bk8WyUBIv1Q6"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from random import shuffle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import struct\n",
    "import csv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "# open the csv file, shuffle, and transform into suitable Attribute and class \n",
    "# vectors\n",
    "\n",
    "wbc = pd.read_csv('bankmarketing.csv')\n",
    "\n",
    "# shuffle\n",
    "wbc = wbc.sample(frac=1)\n",
    "\n",
    "X = wbc.iloc[:,1:9]\n",
    "y = wbc.iloc[:,10]\n",
    "\n",
    "# splitting up training and testing data into appropriate vectors for initial \n",
    "# group\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TGtiZE4lclFs"
   },
   "outputs": [],
   "source": [
    "# INITIAL GROUP PRE-PROCESSING | MLP\n",
    "\n",
    "\n",
    "# Preprocessing - standardize and normalize with sklearn build-in fcn\n",
    "\n",
    "stdScaler = StandardScaler()\n",
    "\n",
    "stdScaler.fit(X_train)\n",
    "\n",
    "X_train = stdScaler.transform(X_train)\n",
    "X_test = stdScaler.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "JELgwOMceHfD",
    "outputId": "7a48ec4b-2897-4ef6-a1db-318a3f3161d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2712  207  225  135    0   73    0    0   21    0    0    0]\n",
      " [ 548  645   46   94    0   16    0    0   23    0    0    0]\n",
      " [ 492   63  683  347    0   93    0    0   35    0    0    0]\n",
      " [ 172   11  218 1108    0   61    0    0   30    0    1    0]\n",
      " [  28    5   38  101    0   16    0    0   13    0    0    0]\n",
      " [ 305   10  253  258    0  138    0    0   24    0    0    0]\n",
      " [   9    0   21   30    0    3    0    0    6    0    0    0]\n",
      " [  99    1   97  118    0   18    0    0   23    0    0    0]\n",
      " [ 193    1  163  218    0   26    0    0   31    0    1    0]\n",
      " [  24    0   19   60    0   16    0    0    9    0    0    0]\n",
      " [ 428    2   99  122    1   68    0    0   14    0    0    0]\n",
      " [  23    7   30   57    0    6    0    0   11    0    1    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.80      0.65      3373\n",
      "           1       0.68      0.47      0.56      1372\n",
      "           2       0.36      0.40      0.38      1713\n",
      "           3       0.42      0.69      0.52      1601\n",
      "           4       0.00      0.00      0.00       201\n",
      "           5       0.26      0.14      0.18       988\n",
      "           6       0.00      0.00      0.00        69\n",
      "           7       0.00      0.00      0.00       356\n",
      "           8       0.13      0.05      0.07       633\n",
      "           9       0.00      0.00      0.00       128\n",
      "          10       0.00      0.00      0.00       734\n",
      "          11       0.00      0.00      0.00       135\n",
      "\n",
      "    accuracy                           0.47     11303\n",
      "   macro avg       0.20      0.21      0.20     11303\n",
      "weighted avg       0.39      0.47      0.41     11303\n",
      "\n",
      "Total MLP model accuracy: 0.4704060868795895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# BUILDING THE INITIAL MLP\n",
    "\n",
    "\n",
    "# I built the Multilayer Perceptron using sklearn\n",
    "\n",
    "# Instantiate MLPClassifier with adjusted max_iter\n",
    "mlp = MLPClassifier(max_iter=20)  \n",
    "\n",
    "# initialize our Multilayer Perceptron\n",
    "nn = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(15,), random_state=1)\n",
    "\n",
    "# fit the neural network model\n",
    "nn.fit(X_train, y_train)\n",
    "\n",
    "#create a vector of the actual predictions\n",
    "predictions = nn.predict(X_test)\n",
    "\n",
    "# print a confusion matrix visual\n",
    "print(confusion_matrix(y_test,predictions))\n",
    "\n",
    "# print a table that identifies prediction outputs\n",
    "print(classification_report(y_test,predictions))\n",
    "\n",
    "print(\"Total MLP model accuracy: \" + str(accuracy_score(y_test, predictions, \n",
    "                                                        normalize=True, \n",
    "                                                        sample_weight=None)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "colab_type": "code",
    "id": "EVe4-RlojBi2",
    "outputId": "f7f9925a-f295-4136-d899-e849da291566"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 50>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m title \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLearning Curve MLP\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     49\u001b[0m estimator \u001b[38;5;241m=\u001b[39m nn\n\u001b[1;32m---> 50\u001b[0m \u001b[43mplot_learning_curve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.01\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36mplot_learning_curve\u001b[1;34m(estimator, title, X, y, ylim, cv, n_jobs, train_sizes)\u001b[0m\n\u001b[0;32m     25\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining examples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m train_sizes, train_scores, test_scores \u001b[38;5;241m=\u001b[39m \u001b[43mlearning_curve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m train_scores_mean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(train_scores, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     30\u001b[0m train_scores_std \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(train_scores, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:1553\u001b[0m, in \u001b[0;36mlearning_curve\u001b[1;34m(estimator, X, y, groups, train_sizes, cv, scoring, exploit_incremental_learning, n_jobs, pre_dispatch, verbose, shuffle, random_state, error_score, return_times, fit_params)\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m n_train_samples \u001b[38;5;129;01min\u001b[39;00m train_sizes_abs:\n\u001b[0;32m   1551\u001b[0m         train_test_proportions\u001b[38;5;241m.\u001b[39mappend((train[:n_train_samples], test))\n\u001b[1;32m-> 1553\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1565\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_times\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_test_proportions\u001b[49m\n\u001b[0;32m   1569\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1570\u001b[0m results \u001b[38;5;241m=\u001b[39m _aggregate_score_dicts(results)\n\u001b[0;32m   1571\u001b[0m train_scores \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, n_unique_ticks)\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    678\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 680\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    683\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    684\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:752\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;124;03m\"\"\"Fit the model to data matrix X and target(s) y.\u001b[39;00m\n\u001b[0;32m    737\u001b[0m \n\u001b[0;32m    738\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;124;03m        Returns a trained MLP model.\u001b[39;00m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 752\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mincremental\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:440\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._fit\u001b[1;34m(self, X, y, incremental)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;66;03m# Run the LBFGS solver\u001b[39;00m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlbfgs\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 440\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_lbfgs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeltas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoef_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintercept_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_units\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:536\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._fit_lbfgs\u001b[1;34m(self, X, y, activations, deltas, coef_grads, intercept_grads, layer_units)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    534\u001b[0m     iprint \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 536\u001b[0m opt_res \u001b[38;5;241m=\u001b[39m \u001b[43mscipy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loss_grad_lbfgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpacked_coef_inter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxfun\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_fun\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxiter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miprint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43miprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgtol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeltas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoef_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintercept_grads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m _check_optimize_result(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlbfgs\u001b[39m\u001b[38;5;124m\"\u001b[39m, opt_res, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter)\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_ \u001b[38;5;241m=\u001b[39m opt_res\u001b[38;5;241m.\u001b[39mfun\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_minimize.py:623\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    621\u001b[0m                               \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m    624\u001b[0m                             callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    627\u001b[0m                          \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py:360\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    354\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[1;32m--> 360\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[0;32m    363\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:267\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x_impl(x)\n\u001b[1;32m--> 267\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad()\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:233\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[1;32m--> 233\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    234\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:134\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py:74\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;124;03m\"\"\" returns the the function value \"\"\"\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\optimize.py:68\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(x \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 68\u001b[0m     fg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:234\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._loss_grad_lbfgs\u001b[1;34m(self, packed_coef_inter, X, y, activations, deltas, coef_grads, intercept_grads)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;124;03m\"\"\"Compute the MLP loss function and its corresponding derivatives\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03mwith respect to the different parameters given in the initialization.\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;124;03mgrad : array-like, shape (number of nodes of all layers,)\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unpack(packed_coef_inter)\n\u001b[1;32m--> 234\u001b[0m loss, coef_grads, intercept_grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backprop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeltas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoef_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintercept_grads\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m grad \u001b[38;5;241m=\u001b[39m _pack(coef_grads, intercept_grads)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss, grad\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:285\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._backprop\u001b[1;34m(self, X, y, activations, deltas, coef_grads, intercept_grads)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss_func_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_activation_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogistic\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    284\u001b[0m     loss_func_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary_log_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 285\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mLOSS_FUNCTIONS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mloss_func_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# Add L2 regularization term to loss\u001b[39;00m\n\u001b[0;32m    287\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_base.py:195\u001b[0m, in \u001b[0;36mlog_loss\u001b[1;34m(y_true, y_prob)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m\"\"\"Compute Logistic loss for classification.\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \n\u001b[0;32m    180\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;124;03m    The degree to which the samples are correctly predicted.\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    194\u001b[0m eps \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfinfo(y_prob\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39meps\n\u001b[1;32m--> 195\u001b[0m y_prob \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_prob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_prob\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    197\u001b[0m     y_prob \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m y_prob, y_prob, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mclip\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2115\u001b[0m, in \u001b[0;36mclip\u001b[1;34m(a, a_min, a_max, out, **kwargs)\u001b[0m\n\u001b[0;32m   2046\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_clip_dispatcher)\n\u001b[0;32m   2047\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclip\u001b[39m(a, a_min, a_max, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   2048\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2049\u001b[0m \u001b[38;5;124;03m    Clip (limit) the values in an array.\u001b[39;00m\n\u001b[0;32m   2050\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2113\u001b[0m \n\u001b[0;32m   2114\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m'\u001b[39m, a_min, a_max, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:159\u001b[0m, in \u001b[0;36m_clip\u001b[1;34m(a, min, max, out, casting, **kwargs)\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _clip_dep_invoke_with_casting(\n\u001b[0;32m    157\u001b[0m         um\u001b[38;5;241m.\u001b[39mmaximum, a, \u001b[38;5;28mmin\u001b[39m, out\u001b[38;5;241m=\u001b[39mout, casting\u001b[38;5;241m=\u001b[39mcasting, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _clip_dep_invoke_with_casting(\n\u001b[0;32m    160\u001b[0m         um\u001b[38;5;241m.\u001b[39mclip, a, \u001b[38;5;28mmin\u001b[39m, \u001b[38;5;28mmax\u001b[39m, out\u001b[38;5;241m=\u001b[39mout, casting\u001b[38;5;241m=\u001b[39mcasting, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:113\u001b[0m, in \u001b[0;36m_clip_dep_invoke_with_casting\u001b[1;34m(ufunc, out, casting, *args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# try to deal with broken casting rules\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ufunc(\u001b[38;5;241m*\u001b[39margs, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _exceptions\u001b[38;5;241m.\u001b[39m_UFuncOutputCastingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;66;03m# Numpy 1.17.0, 2019-02-24\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    117\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting the output of clip from \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    118\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPass `casting=\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m` explicitly to silence this warning, or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    121\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    122\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LEARNING CURVE USING SKLEARN\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# The plot_learning_curve function defined below is from sklearn\n",
    "# Here is the source \n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/\n",
    "# plot_learning_curve.html\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Testing score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "title = \"Learning Curve MLP\"\n",
    "estimator = nn\n",
    "plot_learning_curve(estimator, title, X, y, (0.8, 1.01))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# From the learning curve plot, we can determine that this data is not linearly \n",
    "# separable as the Perceptron does not converge.  This means that we will need\n",
    "# at least one hidden layer.  As one hidden layer will do the job, the number of\n",
    "# neurons in that hidden layer we would use would be as a rule of thumb\n",
    "# around 3 - 5 times the amount of input neurons we have.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#BUILDING THE SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1H3Td07tpnDc",
    "outputId": "842b8f12-0235-4c14-c192-111e608f8a3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total SVM model accuracy:  0.9371428571428572\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "\n",
    "# I built the Support Vector Machine using appropriate functions from sklearn\n",
    "# libraries\n",
    "\n",
    "vectorMachine = svm.SVC(kernel='linear')\n",
    "\n",
    "#Train the model using the training sets\n",
    "vectorMachine.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = vectorMachine.predict(X_test)\n",
    "\n",
    "# Model Accuracy\n",
    "\n",
    "print(\"Total SVM model accuracy: \", metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# The Support Vector Machine is on average more accurate than the MLP \n",
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BiOzIUtGu-8u",
    "outputId": "eaa44b65-5ebb-4761-a929-2f62e20c1499"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 5-fold SVM model accuracy:  0.9903846153846154\n"
     ]
    }
   ],
   "source": [
    "# 5-FOLD CROSS VALIDATION - in order to test my Support Vector Machine accuracy\n",
    "\n",
    "\n",
    "# split training and testing data into appropriate 5-fold vectors\n",
    "\n",
    "X_for_Kfold = np.array(X_train)\n",
    "             \n",
    "y_for_Kfold = np.array(y_train)\n",
    "kf = KFold(n_splits=5)\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "for i, j in kf.split(X_for_Kfold):\n",
    "   X_train2, X_test2 = X_for_Kfold[i], X_for_Kfold[j]\n",
    "   y_train2, y_test2 = y_for_Kfold[i], y_for_Kfold[j]\n",
    "\n",
    "  \n",
    "# pre-process the data\n",
    "\n",
    "stdScaler.fit(X_train2)\n",
    "\n",
    "X_train2 = stdScaler.transform(X_train2)\n",
    "X_test2 = stdScaler.transform(X_test2)\n",
    "\n",
    "#Train the model using the training sets\n",
    "vectorMachine.fit(X_train2, y_train2)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred2 = vectorMachine.predict(X_test2)\n",
    "\n",
    "# Model Accuracy\n",
    "\n",
    "print(\"Total 5-fold SVM model accuracy: \", \n",
    "      metrics.accuracy_score(y_test2, y_pred2))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Comparing MLP and SVM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
